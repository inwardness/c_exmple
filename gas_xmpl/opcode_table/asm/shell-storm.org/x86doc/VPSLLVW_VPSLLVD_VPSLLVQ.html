<!DOCTYPE html>

<html lang="en">

<!-- Mirrored from shell-storm.org/x86doc/VPSLLVW_VPSLLVD_VPSLLVQ.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 12 Sep 2023 17:02:58 GMT -->
<head>
<meta charset="utf-8">
<title>VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical </title>
<meta name="Description" content="VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical " />
<meta content="VPSLLVW/VPSLLVD/VPSLLVQ, x64 opcodes, nasm opcode table, assembly opcode table, intel opcode reference, x86 opcode, instruction reference, assembly opcodes, intel semantics" name="keywords">
<meta name="Viewport" content="width=device-width, initial-scale=1.0"/>
<meta name="Robots" content="index,follow"/>
<link href="style.css" type="text/css" rel="stylesheet">
<script async src="../../www.googletagmanager.com/gtag/jsb2d6?id=G-NLNHL50HG5"></script>
<script src="https://shell-storm.org/assets/js/gtag.js"></script>
</head>
<body><a href="index.html">Back to opcode table</a>
<h1>VPSLLVW/VPSLLVD/VPSLLVQ—Variable Bit Shift Left Logical</h1>
<table>
<tr>
<th>Opcode/Instruction</th>
<th>Op /En</th>
<th>64/32 bit Mode Support</th>
<th>CPUID Feature Flag</th>
<th>Description</th></tr>
<tr>
<td>
<p>VEX.NDS.128.66.0F38.W0 47 /r</p>
<p>VPSLLVD xmm1, xmm2, xmm3/m128</p></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX2</td>
<td>Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.</td></tr>
<tr>
<td>
<p>VEX.NDS.128.66.0F38.W1 47 /r</p>
<p>VPSLLVQ xmm1, xmm2, xmm3/m128</p></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX2</td>
<td>Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s.</td></tr>
<tr>
<td>
<p>VEX.NDS.256.66.0F38.W0 47 /r</p>
<p>VPSLLVD ymm1, ymm2, ymm3/m256</p></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX2</td>
<td>Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.</td></tr>
<tr>
<td>
<p>VEX.NDS.256.66.0F38.W1 47 /r</p>
<p>VPSLLVQ ymm1, ymm2, ymm3/m256</p></td>
<td>RVM</td>
<td>V/V</td>
<td>AVX2</td>
<td>Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s.</td></tr>
<tr>
<td>
<p>EVEX.NDS.128.66.0F38.W1 12 /r</p>
<p>VPSLLVW xmm1 {k1}{z}, xmm2, xmm3/m128</p></td>
<td>FVM</td>
<td>V/V</td>
<td>AVX512VL AVX512BW</td>
<td>Shift words in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.256.66.0F38.W1 12 /r</p>
<p>VPSLLVW ymm1 {k1}{z}, ymm2, ymm3/m256</p></td>
<td>FVM</td>
<td>V/V</td>
<td>AVX512VL AVX512BW</td>
<td>Shift words in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.512.66.0F38.W1 12 /r</p>
<p>VPSLLVW zmm1 {k1}{z}, zmm2, zmm3/m512</p></td>
<td>FVM</td>
<td>V/V</td>
<td>AVX512BW</td>
<td>Shift words in zmm2 left by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.128.66.0F38.W0 47 /r</p>
<p>VPSLLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst</p></td>
<td>FV</td>
<td>V/V</td>
<td>AVX512VL AVX512F</td>
<td>Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.256.66.0F38.W0 47 /r</p>
<p>VPSLLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst</p></td>
<td>FV</td>
<td>V/V</td>
<td>AVX512VL AVX512F</td>
<td>Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.512.66.0F38.W0 47 /r</p>
<p>VPSLLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst</p></td>
<td>FV</td>
<td>V/V</td>
<td>AVX512F</td>
<td>Shift doublewords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.128.66.0F38.W1 47 /r</p>
<p>VPSLLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst</p></td>
<td>FV</td>
<td>V/V</td>
<td>AVX512VL AVX512F</td>
<td>Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.256.66.0F38.W1 47 /r</p>
<p>VPSLLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst</p></td>
<td>FV</td>
<td>V/V</td>
<td>AVX512VL AVX512F</td>
<td>Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1.</td></tr>
<tr>
<td>
<p>EVEX.NDS.512.66.0F38.W1 47 /r</p>
<p>VPSLLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst</p></td>
<td>FV</td>
<td>V/V</td>
<td>AVX512F</td>
<td>Shift quadwords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1.</td></tr></table>
<h3>Instruction Operand Encoding</h3>
<table>
<tr>
<td>Op/En</td>
<td>Operand 1</td>
<td>Operand 2</td>
<td>Operand 3</td>
<td>Operand 4</td></tr>
<tr>
<td>RVM</td>
<td>ModRM:reg (w)</td>
<td>VEX.vvvv (r)</td>
<td>ModRM:r/m (r)</td>
<td>NA</td></tr>
<tr>
<td>FVM</td>
<td>ModRM:reg (w)</td>
<td>EVEX.vvvv (r)</td>
<td>ModRM:r/m (r)</td>
<td>NA</td></tr>
<tr>
<td>FV</td>
<td>ModRM:reg (w)</td>
<td>EVEX.vvvv (r)</td>
<td>ModRM:r/m (r)</td>
<td>NA</td></tr></table>
<h2>Description</h2>
<p>Shifts the bits in the individual data elements (words, doublewords or quadword) in the first source operand to the left by the count value of respective data elements in the second source operand. As the bits in the data elements are shifted left, the empty low-order bits are cleared (set to 0).</p>
<p>The count values are specified individually in each data element of the second source operand. If the unsigned integer value specified in the respective data element of the second source operand is greater than 15 (for word), 31 (for doublewords), or 63 (for a quadword), then the destination data element are written with 0.</p>
<p>VEX.128 encoded version: The destination and first source operands are XMM registers. The count operand can be either an XMM register or a 128-bit memory location. Bits (MAX_VL-1:128) of the corresponding destination register are zeroed.</p>
<p>VEX.256 encoded version: The destination and first source operands are YMM registers. The count operand can be either an YMM register or a 256-bit memory. Bits (MAX_VL-1:256) of the corresponding ZMM register are zeroed.</p>
<p>EVEX encoded VPSLLVD/Q: The destination and first source operands are ZMM/YMM/XMM registers. The count operand can be either a ZMM/YMM/XMM register, a 512/256/128-bit memory location or a 512-bit vector broad-casted from a 32/64-bit memory location. The destination is conditionally updated with writemask k1.</p>
<p>EVEX encoded VPSLLVW: The destination and first source operands are ZMM/YMM/XMM registers. The count operand can be either a ZMM/YMM/XMM register, a 512/256/128-bit memory location. The destination is condition-ally updated with writemask k1.</p>
<h2>Operation</h2>
<pre>
</pre>
<strong>VPSLLVW (EVEX encoded version)</strong>
<pre>
(KL, VL) = (8, 128), (16, 256), (32, 512)
FOR j (cid:197) 0 TO KL-1
    i (cid:197) j * 16
    IF k1[j] OR *no writemask*
        THEN DEST[i+15:i] (cid:197) ZeroExtend(SRC1[i+15:i] &lt;&lt; SRC2[i+15:i])
        ELSE
        IF *merging-masking*
            ; merging-masking
            THEN *DEST[i+15:i] remains unchanged*
            ELSE
            ; zeroing-masking
            DEST[i+15:i] (cid:197) 0
        FI
    FI;
ENDFOR;
DEST[MAX_VL-1:VL] (cid:197) 0;
</pre>
<strong>VPSLLVD (VEX.128 version)</strong>
<pre>
COUNT_0 (cid:197)SRC2[31 : 0]
(* Repeat Each COUNT_i for the 2nd through 4th dwords of SRC2*)
COUNT_3 (cid:197)SRC2[100 : 96];
IF COUNT_0 &lt; 32 THEN
    DEST[31:0] (cid:197)ZeroExtend(SRC1[31:0] &lt;&lt; COUNT_0);
    ELSE
    DEST[31:0] (cid:197)0;
    (* Repeat shift operation for 2nd through 4th dwords *)
    IF COUNT_3 &lt; 32 THEN
        DEST[127:96] (cid:197)ZeroExtend(SRC1[127:96] &lt;&lt; COUNT_3);
        ELSE
        DEST[127:96] (cid:197)0;
        DEST[MAX_VL-1:128] (cid:197)0;
</pre>
<strong>VPSLLVD (VEX.256 version)</strong>
<pre>
COUNT_0 (cid:197)SRC2[31 : 0];
(* Repeat Each COUNT_i for the 2nd through 7th dwords of SRC2*)
COUNT_7 (cid:197)SRC2[228 : 224];
IF COUNT_0 &lt; 32 THEN
    DEST[31:0] (cid:197)ZeroExtend(SRC1[31:0] &lt;&lt; COUNT_0);
    ELSE
    DEST[31:0] (cid:197)0;
    (* Repeat shift operation for 2nd through 7th dwords *)
    IF COUNT_7 &lt; 32 THEN
        DEST[255:224] (cid:197)ZeroExtend(SRC1[255:224] &lt;&lt; COUNT_7);
        ELSE
        DEST[255:224] (cid:197)0;
        DEST[MAX_VL-1:256] (cid:197) 0;
</pre>
<strong>VPSLLVD (EVEX encoded version)</strong>
<pre>
(KL, VL) = (4, 128), (8, 256), (16, 512)
FOR j (cid:197) 0 TO KL-1
    i (cid:197) j * 32
    IF k1[j] OR *no writemask* THEN
        IF (EVEX.b = 1) AND (SRC2 *is memory*)
            THEN DEST[i+31:i] (cid:197) ZeroExtend(SRC1[i+31:i] &lt;&lt; SRC2[31:0])
            ELSE DEST[i+31:i] (cid:197) ZeroExtend(SRC1[i+31:i] &lt;&lt; SRC2[i+31:i])
        FI;
        ELSE
        IF *merging-masking*
            ; merging-masking
            THEN *DEST[i+31:i] remains unchanged*
            ELSE
            ; zeroing-masking
            DEST[i+31:i] (cid:197) 0
        FI
    FI;
ENDFOR;
DEST[MAX_VL-1:VL] (cid:197) 0;
</pre>
<strong>VPSLLVQ (VEX.128 version)</strong>
<pre>
COUNT_0 (cid:197)SRC2[63 : 0];
COUNT_1 (cid:197)SRC2[127 : 64];
IF COUNT_0 &lt; 64THEN
    DEST[63:0] (cid:197)ZeroExtend(SRC1[63:0] &lt;&lt; COUNT_0);
    ELSE
    DEST[63:0] (cid:197)0;
    IF COUNT_1 &lt; 64 THEN
        DEST[127:64] (cid:197)ZeroExtend(SRC1[127:64] &lt;&lt; COUNT_1);
        ELSE
        DEST[127:96] (cid:197)0;
        DEST[MAX_VL-1:128] (cid:197)0;
</pre>
<strong>VPSLLVQ (VEX.256 version)</strong>
<pre>
COUNT_0 (cid:197)SRC2[63 : 0];
(* Repeat Each COUNT_i for the 2nd through 4th dwords of SRC2*)
COUNT_3 (cid:197)SRC2[197 : 192];
IF COUNT_0 &lt; 64THEN
    DEST[63:0] (cid:197)ZeroExtend(SRC1[63:0] &lt;&lt; COUNT_0);
    ELSE
    DEST[63:0] (cid:197)0;
    (* Repeat shift operation for 2nd through 4th dwords *)
    IF COUNT_3 &lt; 64 THEN
        DEST[255:192] (cid:197)ZeroExtend(SRC1[255:192] &lt;&lt; COUNT_3);
        ELSE
        DEST[255:192] (cid:197)0;
        DEST[MAX_VL-1:256] (cid:197) 0;
</pre>
<strong>VPSLLVQ (EVEX encoded version)</strong>
<pre>
(KL, VL) = (2, 128), (4, 256), (8, 512)
FOR j (cid:197) 0 TO KL-1
    i (cid:197) j * 64
    IF k1[j] OR *no writemask* THEN
        IF (EVEX.b = 1) AND (SRC2 *is memory*)
            THEN DEST[i+63:i] (cid:197) ZeroExtend(SRC1[i+63:i] &lt;&lt; SRC2[63:0])
            ELSE DEST[i+63:i] (cid:197) ZeroExtend(SRC1[i+63:i] &lt;&lt; SRC2[i+63:i])
        FI;
        ELSE
        IF *merging-masking*
            ; merging-masking
            THEN *DEST[i+63:i] remains unchanged*
            ELSE
            ; zeroing-masking
            DEST[i+63:i] (cid:197) 0
        FI
    FI;
ENDFOR;
DEST[MAX_VL-1:VL] (cid:197) 0;
</pre>
<h2>Intel C/C++ Compiler Intrinsic Equivalent</h2>
<pre>
VPSLLVW __m512i _mm512_sllv_epi16(__m512i a, __m512i cnt);
VPSLLVW __m512i _mm512_mask_sllv_epi16(__m512i s, __mmask32 k, __m512i a, __m512i cnt);
VPSLLVW __m512i _mm512_maskz_sllv_epi16( __mmask32 k, __m512i a, __m512i cnt);
VPSLLVW __m256i _mm256_mask_sllv_epi16(__m256i s, __mmask16 k, __m256i a, __m256i cnt);
VPSLLVW __m256i _mm256_maskz_sllv_epi16( __mmask16 k, __m256i a, __m256i cnt);
VPSLLVW __m128i _mm_mask_sllv_epi16(__m128i s, __mmask8 k, __m128i a, __m128i cnt);
VPSLLVW __m128i _mm_maskz_sllv_epi16( __mmask8 k, __m128i a, __m128i cnt);
VPSLLVD __m512i _mm512_sllv_epi32(__m512i a, __m512i cnt);
VPSLLVD __m512i _mm512_mask_sllv_epi32(__m512i s, __mmask16 k, __m512i a, __m512i cnt);
VPSLLVD __m512i _mm512_maskz_sllv_epi32( __mmask16 k, __m512i a, __m512i cnt);
VPSLLVD __m256i _mm256_mask_sllv_epi32(__m256i s, __mmask8 k, __m256i a, __m256i cnt);
VPSLLVD __m256i _mm256_maskz_sllv_epi32( __mmask8 k, __m256i a, __m256i cnt);
VPSLLVD __m128i _mm_mask_sllv_epi32(__m128i s, __mmask8 k, __m128i a, __m128i cnt);
VPSLLVD __m128i _mm_maskz_sllv_epi32( __mmask8 k, __m128i a, __m128i cnt);
VPSLLVQ __m512i _mm512_sllv_epi64(__m512i a, __m512i cnt);
VPSLLVQ __m512i _mm512_mask_sllv_epi64(__m512i s, __mmask8 k, __m512i a, __m512i cnt);
VPSLLVQ __m512i _mm512_maskz_sllv_epi64( __mmask8 k, __m512i a, __m512i cnt);
VPSLLVD __m256i _mm256_mask_sllv_epi64(__m256i s, __mmask8 k, __m256i a, __m256i cnt);
VPSLLVD __m256i _mm256_maskz_sllv_epi64( __mmask8 k, __m256i a, __m256i cnt);
VPSLLVD __m128i _mm_mask_sllv_epi64(__m128i s, __mmask8 k, __m128i a, __m128i cnt);
VPSLLVD __m128i _mm_maskz_sllv_epi64( __mmask8 k, __m128i a, __m128i cnt);
VPSLLVD __m256i _mm256_sllv_epi32 (__m256i m, __m256i count)
VPSLLVQ __m256i _mm256_sllv_epi64 (__m256i m, __m256i count)
</pre>
<h2>SIMD Floating-Point Exceptions</h2>
<p>None</p>
<h2>Other Exceptions</h2>
<table>
<tr>
<td>VEX-encoded instructions, see Exceptions Type 4.</td></tr>
<tr>
<td>EVEX-encoded VPSLLVD/VPSLLVQ, see Exceptions Type E4.</td></tr>
<tr>
<td>EVEX-encoded VPSLLVW, see Exceptions Type E4.nb.</td></tr></table>
</body>

<!-- Mirrored from shell-storm.org/x86doc/VPSLLVW_VPSLLVD_VPSLLVQ.html by HTTrack Website Copier/3.x [XR&CO'2014], Tue, 12 Sep 2023 17:02:58 GMT -->
</html>
